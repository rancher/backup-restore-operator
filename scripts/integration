#!/bin/bash
set -ex
function echo_with_time {
    echo "$(date --utc +%Y-%m-%dT%H:%M:%SZ) "$@""
}

if [ "$ARCH" != "amd64" ]; then
    echo_with_time "Integration test only run on amd64"
    exit 0
fi

source $(dirname $0)/version

echo_with_time "INFO - Running $0"

cd $(dirname $0)/..

k3s server --disable servicelb --disable traefik --disable local-storage --disable metrics-server > /tmp/k3s.log 2>&1 &
k3s_pid=$!

export KUBECONFIG=/etc/rancher/k3s/k3s.yaml

echo_with_time 'Waiting for node to be ready ...'
time timeout 300 bash -c 'while ! (k3s kubectl wait --for condition=ready node/$(hostname) 2>/dev/null); do sleep 5; done'
time timeout 300 bash -c 'while ! (k3s kubectl --namespace kube-system rollout status --timeout 10s deploy/coredns 2>/dev/null); do sleep 5; done'

k3s kubectl get nodes -o wide

docker image save rancher/backup-restore-operator:$TAG -o /tmp/bro.img

k3s ctr images import /tmp/bro.img

ls -la ./dist/artifacts

# In case short commit only conists of numbers, it is regarded valid by Helm when packaging
# Or if a tag is set (if its a (pre) release)
if [[ $HELM_VERSION =~ ^[0-9]+$ ]] || [[ -n $GIT_TAG ]]; then
  HELM_CHART_VERSION=$HELM_VERSION
else
  HELM_CHART_VERSION=$HELM_VERSION_DEV
fi

helm install rancher-backup-crd ./dist/artifacts/rancher-backup-crd-$HELM_CHART_VERSION.tgz -n cattle-resources-system --create-namespace --wait
helm install rancher-backup ./dist/artifacts/rancher-backup-$HELM_CHART_VERSION.tgz -n cattle-resources-system --set image.tag=$TAG --set imagePullPolicy=IfNotPresent

time timeout 300 bash -c 'while ! (k3s kubectl --namespace cattle-resources-system rollout status --timeout 10s deploy/rancher-backup 2>/dev/null); do sleep 5; done'

k3s kubectl -n cattle-resources-system get deploy rancher-backup -o yaml

#Deploy Minio
./scripts/deploy minio

#Create Backup
./scripts/deploy create-backup

time timeout 60 bash -c 'while ! (k3s kubectl wait --for condition=ready backup.resources.cattle.io/s3-recurring-backup 2>/dev/null); do k3s kubectl get backup.resources.cattle.io -A; sleep 2; done'

k3s kubectl -n cattle-resources-system logs -l app.kubernetes.io/name=rancher-backup --tail=-1

export POD_NAME=$(k3s kubectl get pods --namespace minio -l "release=minio" -o jsonpath="{.items[0].metadata.name}")
k3s kubectl port-forward $POD_NAME 9000 --namespace minio &
sleep 10

mkdir -p $HOME/.mc/certs/CAs
PUBLIC_CRT=$(cat <<EOF
-----BEGIN CERTIFICATE-----
MIICSTCCAe+gAwIBAgIQWgUVWCiZdyOGruNe6m4iWjAKBggqhkjOPQQDAjBMMRww
GgYDVQQKExNDZXJ0Z2VuIERldmVsb3BtZW50MSwwKgYDVQQLDCNlbGl5YW1sZXZ5
QEVsaXlhbXMtTUJQLmF0dGxvY2FsLm5ldDAeFw0yMjA1MTExNDAxMjBaFw0zMjA1
MTEwMjAxMjBaMEwxHDAaBgNVBAoTE0NlcnRnZW4gRGV2ZWxvcG1lbnQxLDAqBgNV
BAsMI2VsaXlhbWxldnlARWxpeWFtcy1NQlAuYXR0bG9jYWwubmV0MFkwEwYHKoZI
zj0CAQYIKoZIzj0DAQcDQgAEFH8UPCl/vAHkMbTF3E8yhSdLNH2XueKUHns+O4FR
hn096OJKnGZFb/HiW9iJWhj4CJ4LubSvsiZJZ7YuDlM9faOBsjCBrzAOBgNVHQ8B
Af8EBAMCAqQwEwYDVR0lBAwwCgYIKwYBBQUHAwEwDwYDVR0TAQH/BAUwAwEB/zAd
BgNVHQ4EFgQUImjresql78fBpwSV7lp4fT4+NnwwWAYDVR0RBFEwT4IFbWluaW+C
C21pbmlvLm1pbmlvgg9taW5pby5taW5pby5zdmOCHW1pbmlvLm1pbmlvLnN2Yy5j
bHVzdGVyLmxvY2Fsgglsb2NhbGhvc3QwCgYIKoZIzj0EAwIDSAAwRQIgWT4CU5ib
LNeXJmh2lnqEvaeKgqLHPFgMOQg+4TyO+uQCIQCI5WX1E84B+z6yX7WKIBYJIjto
RjQi75QniF10pi2jKA==
-----END CERTIFICATE-----
EOF
)

echo "$PUBLIC_CRT" > $HOME/.mc/certs/CAs/public.crt
export MC_HOST_miniolocal=https://inspectorgadget:gogadgetgo@localhost:9000
mc ls --quiet --no-color miniolocal/rancherbackups
FIRSTBACKUP=$(mc ls --quiet --no-color miniolocal/rancherbackups | awk '{ print $NF }')
if [[ $FIRSTBACKUP != s3-recurring-backup* ]]; then
    echo_with_time "$FIRSTBACKUP does not start with 's3-recurring-backup'"
    exit 1
fi
sleep 90
for BACKUP in $(mc ls --quiet --no-color miniolocal/rancherbackups | awk '{ print $NF }'); do
    echo_with_time $BACKUP
    if [[ $BACKUP != s3-recurring-backup* ]]; then
        echo_with_time "$BACKUP does not start with 's3-recurring-backup'"
        exit 1
    fi

    if [ "${BACKUP}" = "${FIRSTBACKUP}" ]; then
        echo_with_time "First was not removed!"
	exit 1
    fi
done

# Disable the recurring back-ups by deleting the backup CRD
k3s kubectl delete backup.resources.cattle.io/s3-recurring-backup

# Restore resource with spec.preserveUnknownFields
# https://github.com/rancher/backup-restore-operator/issues/186

cd tests/files/preserve-unknown-fields
tar cvzf /tmp/preserve-unknown-fields.tar.gz -- *
cd -
mc cp --quiet --no-color /tmp/preserve-unknown-fields.tar.gz miniolocal/rancherbackups

mc ls --quiet --no-color miniolocal/rancherbackups

k3s kubectl create -f - <<EOF
apiVersion: resources.cattle.io/v1
kind: Restore
metadata:
  name: restore-preserve-unknown-fields
spec:
  backupFilename: preserve-unknown-fields.tar.gz
  prune: false
  storageLocation:
    s3:
      credentialSecretName: miniocreds
      credentialSecretNamespace: default
      bucketName: rancherbackups
      endpoint: minio.minio.svc.cluster.local:9000
      endpointCA: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUNTVENDQWUrZ0F3SUJBZ0lRV2dVVldDaVpkeU9HcnVOZTZtNGlXakFLQmdncWhrak9QUVFEQWpCTU1Sd3cKR2dZRFZRUUtFeE5EWlhKMFoyVnVJRVJsZG1Wc2IzQnRaVzUwTVN3d0tnWURWUVFMRENObGJHbDVZVzFzWlhaNQpRRVZzYVhsaGJYTXRUVUpRTG1GMGRHeHZZMkZzTG01bGREQWVGdzB5TWpBMU1URXhOREF4TWpCYUZ3MHpNakExCk1URXdNakF4TWpCYU1Fd3hIREFhQmdOVkJBb1RFME5sY25SblpXNGdSR1YyWld4dmNHMWxiblF4TERBcUJnTlYKQkFzTUkyVnNhWGxoYld4bGRubEFSV3hwZVdGdGN5MU5RbEF1WVhSMGJHOWpZV3d1Ym1WME1Ga3dFd1lIS29aSQp6ajBDQVFZSUtvWkl6ajBEQVFjRFFnQUVGSDhVUENsL3ZBSGtNYlRGM0U4eWhTZExOSDJYdWVLVUhucytPNEZSCmhuMDk2T0pLbkdaRmIvSGlXOWlKV2hqNENKNEx1YlN2c2laSlo3WXVEbE05ZmFPQnNqQ0JyekFPQmdOVkhROEIKQWY4RUJBTUNBcVF3RXdZRFZSMGxCQXd3Q2dZSUt3WUJCUVVIQXdFd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZApCZ05WSFE0RUZnUVVJbWpyZXNxbDc4ZkJwd1NWN2xwNGZUNCtObnd3V0FZRFZSMFJCRkV3VDRJRmJXbHVhVytDCkMyMXBibWx2TG0xcGJtbHZnZzl0YVc1cGJ5NXRhVzVwYnk1emRtT0NIVzFwYm1sdkxtMXBibWx2TG5OMll5NWoKYkhWemRHVnlMbXh2WTJGc2dnbHNiMk5oYkdodmMzUXdDZ1lJS29aSXpqMEVBd0lEU0FBd1JRSWdXVDRDVTVpYgpMTmVYSm1oMmxucUV2YWVLZ3FMSFBGZ01PUWcrNFR5Tyt1UUNJUUNJNVdYMUU4NEIrejZ5WDdXS0lCWUpJanRvClJqUWk3NVFuaUYxMHBpMmpLQT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
EOF

time timeout 60 bash -c 'while ! (k3s kubectl wait --for condition=ready restore.resources.cattle.io/restore-preserve-unknown-fields 2>/dev/null); do k3s kubectl get restore.resources.cattle.io -A; k3s kubectl -n cattle-resources-system logs -l app.kubernetes.io/name=rancher-backup --tail=15; sleep 5; done'

# Restore resource with metadata.deletionGracePeriodSeconds
# https://github.com/rancher/backup-restore-operator/issues/188

cd tests/files/deletion-grace-period-seconds
tar cvzf /tmp/deletion-grace-period-seconds.tar.gz -- *
cd -
mc cp --quiet --no-color /tmp/deletion-grace-period-seconds.tar.gz miniolocal/rancherbackups

mc ls --quiet --no-color miniolocal/rancherbackups

# Run this twice as the error happens when the to be restored resource already exists
for i in $(seq 1 2); do
    echo "Running restore #${i} with resource having metadata.deletionGracePeriodSeconds"

    k3s kubectl create -f - <<EOF
apiVersion: resources.cattle.io/v1
kind: Restore
metadata:
  name: restore-deletion-grace-period-seconds
spec:
  backupFilename: deletion-grace-period-seconds.tar.gz
  prune: false
  storageLocation:
    s3:
      credentialSecretName: miniocreds
      credentialSecretNamespace: default
      bucketName: rancherbackups
      endpoint: minio.minio.svc.cluster.local:9000
      endpointCA: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUNTVENDQWUrZ0F3SUJBZ0lRV2dVVldDaVpkeU9HcnVOZTZtNGlXakFLQmdncWhrak9QUVFEQWpCTU1Sd3cKR2dZRFZRUUtFeE5EWlhKMFoyVnVJRVJsZG1Wc2IzQnRaVzUwTVN3d0tnWURWUVFMRENObGJHbDVZVzFzWlhaNQpRRVZzYVhsaGJYTXRUVUpRTG1GMGRHeHZZMkZzTG01bGREQWVGdzB5TWpBMU1URXhOREF4TWpCYUZ3MHpNakExCk1URXdNakF4TWpCYU1Fd3hIREFhQmdOVkJBb1RFME5sY25SblpXNGdSR1YyWld4dmNHMWxiblF4TERBcUJnTlYKQkFzTUkyVnNhWGxoYld4bGRubEFSV3hwZVdGdGN5MU5RbEF1WVhSMGJHOWpZV3d1Ym1WME1Ga3dFd1lIS29aSQp6ajBDQVFZSUtvWkl6ajBEQVFjRFFnQUVGSDhVUENsL3ZBSGtNYlRGM0U4eWhTZExOSDJYdWVLVUhucytPNEZSCmhuMDk2T0pLbkdaRmIvSGlXOWlKV2hqNENKNEx1YlN2c2laSlo3WXVEbE05ZmFPQnNqQ0JyekFPQmdOVkhROEIKQWY4RUJBTUNBcVF3RXdZRFZSMGxCQXd3Q2dZSUt3WUJCUVVIQXdFd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZApCZ05WSFE0RUZnUVVJbWpyZXNxbDc4ZkJwd1NWN2xwNGZUNCtObnd3V0FZRFZSMFJCRkV3VDRJRmJXbHVhVytDCkMyMXBibWx2TG0xcGJtbHZnZzl0YVc1cGJ5NXRhVzVwYnk1emRtT0NIVzFwYm1sdkxtMXBibWx2TG5OMll5NWoKYkhWemRHVnlMbXh2WTJGc2dnbHNiMk5oYkdodmMzUXdDZ1lJS29aSXpqMEVBd0lEU0FBd1JRSWdXVDRDVTVpYgpMTmVYSm1oMmxucUV2YWVLZ3FMSFBGZ01PUWcrNFR5Tyt1UUNJUUNJNVdYMUU4NEIrejZ5WDdXS0lCWUpJanRvClJqUWk3NVFuaUYxMHBpMmpLQT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
EOF

    time timeout 60 bash -c 'while ! (k3s kubectl wait --for condition=ready restore.resources.cattle.io/restore-deletion-grace-period-seconds 2>/dev/null); do k3s kubectl get restore.resources.cattle.io -A; k3s kubectl -n cattle-resources-system logs -l app.kubernetes.io/name=rancher-backup --tail=15; sleep 5; done'
    k3s kubectl delete restore.resources.cattle.io/restore-deletion-grace-period-seconds
done
